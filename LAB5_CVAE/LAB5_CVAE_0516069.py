from __future__ import unicode_literals, print_function, division
from io import open
import unicodedata
import string
import re
import random
import time
import math
import torch
import torch.nn as nn
from torch import optim
from torch.utils.data import Dataset, DataLoader
import torch.nn.functional as F
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
import numpy as np
from os import system
from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

"""==============================================================================
The sample.py includes the following template functions:

1. Encoder, decoder
2. Training function
3. BLEU-4 score function

You have to modify them to complete the lab.
In addition, there are still other functions that you have to 
implement by yourself.

1. The reparameterization trick
2. Your own dataloader (design in your own way, not necessary Pytorch Dataloader)
3. Output your results (BLEU-4 score, words)
4. Plot loss/score
5. Load/save weights

There are some useful tips listed in the lab assignment.
You should check them before starting your lab.
================================================================================"""

SOS_token = 0
EOS_token = 1
#----------Hyper Parameters----------#
hidden_size = 256
#The number of vocabulary
word_size = 28
teacher_forcing_ratio = 0.5
empty_input_ratio = 0.1
KLD_weight = 0.0
LR = 0.05
MAX_LENGTH = 10
condition_num = 4
latent_size = 32
condition_size = 8


#The target word
reference = 'accessed'
#The word generated by your model
output = 'access'

def asMinutes(s):
    m = math.floor(s / 60)
    s -= m * 60
    return '%dm %ds' % (m, s)


def timeSince(since, percent):
    now = time.time()
    s = now - since
    es = s / (percent)
    rs = es - s
    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))

# compute BLEU-4 score
def compute_bleu(output, reference):
    cc = SmoothingFunction()
    return sentence_bleu([reference], output,weights=(0.25, 0.25, 0.25, 0.25),smoothing_function=cc.method1)


# compute kl divergence
def KL_loss(m, logvar):
    return torch.sum(0.5 * (-logvar + (m**2) + torch.exp(logvar) - 1))
	
class CharDict:
    def __init__(self):
        self.word2index = {}
        self.index2word = {}
        self.word_count = 0
        
        for i in range(26):
            self.addWord(chr(ord('a') + i))
        
        tokens = ["SOS", "EOS"]
        for t in tokens:
            self.addWord(t)

    def addWord(self, word):
        if word not in self.word2index:
            self.word2index[word] = self.word_count
            self.index2word[self.word_count] = word
            self.word_count += 1

    def longtensorFromString(self, s):
        s = ["SOS"] + list(s) + ["EOS"]
        return torch.LongTensor([self.word2index[ch] for ch in s])
    
    def stringFromLongtensor(self, l, show_token=False, check_end=True):
        s = ""
        for i in l:
            ch = self.index2word[i.item()]
            if len(ch) > 1:
                if show_token:
                    __ch = "<{}>".format(ch)
                else:
                    __ch = ""
            else:
                __ch = ch
            s += __ch
            if check_end and ch == "EOS":
                break
        return s
		
class DataLoader(Dataset):
    def __init__(self, train = True):
        self.dic = CharDict()
        self.train = train
        if self.train:
            f = './train.txt'
            self.datas = np.loadtxt(f, dtype = np.str)
            self.datas = self.datas.reshape(-1)
        else:
            f = './test.txt'
            self.datas = np.loadtxt(f, dtype = np.str)
            ## [0]simple present(sp), [1]third person(tp), [2]present progressive(pg), [3]simple past(p)
            self.testdata = np.array([
                [0, 3],
                [0, 2],
                [0, 1],
                [0, 1],
                [3, 1],
                [0, 2],
                [3, 0],
                [2, 0],
                [2, 3],
                [2, 1],
            ])
        
    def __len__(self):
        return len(self.datas)
    
    def __getitem__(self, index):
        if self.train:
            return self.dic.longtensorFromString(self.datas[index]), index % 4
        else:
            i = self.dic.longtensorFromString(self.datas[index, 0])
            ci = self.testdata[index, 0]
            o = self.dic.longtensorFromString(self.datas[index, 1])
            co = self.testdata[index, 1]
            
            return i, ci, o, co
			
#Encoder
class EncoderRNN(nn.Module):
    def __init__(self, word_size, hidden_size, latent_size, condition_num, condition_size):
        super(EncoderRNN, self).__init__()
        self.word_size = word_size
        self.hidden_size = hidden_size
        self.condition_size = condition_size
        self.latent_size = latent_size

        self.condition_embedding = nn.Embedding(condition_num, condition_size)
        self.word_embedding = nn.Embedding(word_size, hidden_size)
        self.gru = nn.GRU(hidden_size, hidden_size)
        self.mean = nn.Linear(hidden_size, latent_size)
        self.logvar = nn.Linear(hidden_size, latent_size)

    def forward(self, input, input_c):
        c = torch.LongTensor([input_c]).to(device)
        c = self.condition_embedding(c).view(1,1,-1)
        
        hidden = self.initHidden()
        hidden = torch.cat((hidden, c), dim=2)
        
        x = self.word_embedding(input).view(-1,1, self.hidden_size)
        
        outputs, hidden = self.gru(x, hidden)
        
        m = self.mean(hidden)
        logvar = self.logvar(hidden)
        
        z = self.sample_z() * torch.exp(logvar/2) + m
        
        return z, m, logvar

    def initHidden(self):
        return torch.zeros(1, 1, self.hidden_size - self.condition_size, device=device)
    
    def sample_z(self):
        return torch.normal(
            torch.FloatTensor([0]*self.latent_size), 
            torch.FloatTensor([1]*self.latent_size)
        ).to(device)
		
#Decoder
class DecoderRNN(nn.Module):
    def __init__(self, word_size, hidden_size, latent_size, condition_num, condition_size):
        super(DecoderRNN, self).__init__()
        self.word_size = word_size
        self.hidden_size = hidden_size
       
        
        self.word_embedding = nn.Embedding(word_size, hidden_size)
        self.latent2hidden = nn.Linear(40,256)
        self.condition_embedding = nn.Embedding(4,8)
        self.gru = nn.GRU(hidden_size, hidden_size)
        self.out = nn.Linear(hidden_size, word_size)

    def forward(self, x, hidden):
        x = self.word_embedding(x).view(1,1, self.hidden_size)
        
        output, hidden = self.gru(x, hidden)
        
        output = self.out(output).view(-1, self.word_size)
       
        return output, hidden
    def condition(self,c):
        c = torch.LongTensor([c]).to(device)
        return self.condition_embedding(c).view(1,1,-1)
    def latenthidden(self, z, c):
        latent = torch.cat((z,c), dim=2)
        return self.latent2hidden(latent)
		
def Decoder(decoder, z, c, teacher = False, input = None):
    sos_token = train_data.dic.word2index['SOS']
    eos_token = train_data.dic.word2index['EOS']
    c = decoder.condition(c)
    z = z.view(1,1,-1)
    
    outputs=[]
    if input is not None:
        length = input.size(0)-1
    else:
        length = 15
    x = torch.LongTensor([sos_token]).to(device)
    hidden = decoder.latenthidden(z,c)
    for i in range(length):
        x = x.detach()
        output, hidden = decoder(x, hidden)
        
        outputs.append(output)
        output_onehot = torch.max(torch.softmax(output, dim=1),1)[1]
        
        if output_onehot.item() == eos_token and not teacher:
            break
        
        if teacher:
            x = input[i+1:i+2]
        else:
            x = output_onehot
    if len(outputs) != 0:
        outputs = torch.cat(outputs, dim=0)
    else:
        outputs = torch.FloatTensor([]).view(0, word_size).to(device)
    
    return outputs
	
def test(encoder, decoder, test_data):
    encoder.eval()
    decoder.eval()
    score = 0
    for i in range(10):
        inputs, input_c, output, output_c = test_data[i]
        
        z, m, logvar = encoder(inputs[1:-1].to(device), input_c)
        
        outputs = Decoder(decoder, z, output_c, False, output.to(device))
        
        outputs_onehot = torch.max(torch.softmax(outputs, dim=1), 1)[1]
        inputs_str = train_data.dic.stringFromLongtensor(inputs, check_end=True)
        target_str = train_data.dic.stringFromLongtensor(output, check_end=True)
        outputs_str = train_data.dic.stringFromLongtensor(outputs_onehot, check_end=True)
        
        print('{:12s}->{:12s} and the pred output:{}'.format(inputs_str, target_str, outputs_str))
        
        score += compute_bleu(outputs_str, target_str)
    
    score = score/10
    print('bleu score : %f'%(score))
    
    return score
	
def train(encoder, decoder, train_data, test_data):
    plot_total_loss = []
    plot_loss = []
    plot_kl_loss = []
    plot_score = []
    
    encoder_optimizer = optim.SGD(encoder.parameters(), lr=0.01)
    decoder_optimizer = optim.SGD(decoder.parameters(), lr=0.01)
    
    loss_function = nn.CrossEntropyLoss()
    kl_loss_function = nn.KLDivLoss()
    
    blue4_score = 0.0
    
    for epoch in range(100):
        encoder.train()
        decoder.train()
        total_loss = 0
        loss = 0
        kl_loss = 0
        
        for index in range(len(train_data)):
            
            input, c = train_data[index]
            
            encoder_optimizer.zero_grad()
            decoder_optimizer.zero_grad()
            
            z, m, logvar = encoder(input[1:-1].to(device), c)
            
            teacher_forcing_ratio = 1 - epoch*0.005
            if teacher_forcing_ratio < 0:
                teacher_forcing_ration = 0
            
            use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False
            
            output = Decoder(decoder, z, c, use_teacher_forcing, input = input.to(device))
            
            train_loss = loss_function(output, input[1:1+output.size(0)].to(device))
            train_kl_loss = KL_loss(m, logvar)
            train_total_loss = train_loss + train_kl_loss
            
            kl_w = (epoch%100)*0.001
            if kl_w>1.0:
                kl_w = 1.0
            
            (train_loss + kl_w * train_kl_loss).backward()
            
            encoder_optimizer.step()
            decoder_optimizer.step()
            
            total_loss += train_loss.item() + kl_w*train_kl_loss.item()
            kl_loss += train_kl_loss.item()
            loss += train_loss.item()
            
        score = test(encoder, decoder, test_data)
        if score > blue4_score:
            print("saving...")
            torch.save(encoder.state_dict(), 'encoder.pkl')
            torch.save(decoder.state_dict(), 'decoder.pkl')
            blue4_score = score
        plot_score.append(score)
        plot_total_loss.append(total_loss/4908)
        plot_kl_loss.append(kl_loss/4908)
        plot_loss.append(loss/4908)
        
        print("Epoch      :{}".format(epoch))
        print("Total_loss :{}".format(total_loss/4908))
        print("KL_loss    :{}".format(kl_loss/4908))
        print("loss       :{}".format(loss/4908))
        z = encoder.sample_z()
        for c in range(4):
            gen_out = generate_word(decoder,z,c)
            gen_word = train_data.dic.stringFromLongtensor(gen_out)
            print("condition %d : %s"%(c,gen_word))
            
        print("-----------------------------------------------")
    return plot_score, plot_total_loss, plot_kl_loss, plot_loss
	
def generate_word(decoder,z,c):
    decoder.eval()
    outputs = Decoder(decoder,z,c)
    return torch.max(torch.softmax(outputs, dim=1), 1)[1]
	
train_data = DataLoader()
test_data = DataLoader(False)

encoder = EncoderRNN(word_size, hidden_size, latent_size, condition_num, condition_size).to(device)
decoder = DecoderRNN(word_size, hidden_size, latent_size, condition_num, condition_size).to(device)

plot_score, plot_total_loss, plot_kl_loss, plot_loss=train(encoder, decoder, train_data, test_data)

x_axix = range(100)
plt.subplot(2,1,1)
plt.title('Traing loss')
plt.plot(x_axix, plot_total_loss, color = "pink", label = 'total_loss')
plt.plot(x_axix, plot_kl_loss, color = "blue", label = 'KL_loss')
plt.plot(x_axix, plot_loss, color = "green", label = 'loss')
plt.legend()
plt.xlabel('Epoch')
plt.ylabel('loss')
    
plt.subplot(2,1,2)
plt.title('Testing Bleu score')
plt.plot(x_axix, plot_score, color = "red")
plt.xlabel('Epoch')
plt.ylabel('score')
plt.subplots_adjust(hspace = 0.6)

plt.show()

decoder.load_state_dict(torch.load('decoder.pkl'))
encoder.load_state_dict(torch.load('encoder.pkl'))

noise = torch.FloatTensor([-0.7582,  0.1164, -0.9150,  0.0746,  0.4415,  0.2936, -0.2462, -2.2326,
         0.3443,  0.0221,  1.3298, -0.1673,  1.1407, -1.0491, -1.4391,  1.0493,
         2.3256,  2.0457,  0.1153,  1.1268, -0.7038,  0.7340, -1.9563, -0.8247,
         0.4381,  0.5817, -0.2629,  0.0147, -1.4602,  0.0104, -0.9053, -1.2561]).to(device)
for c in range(4):
    gen_out = generate_word(decoder,noise,c)
    gen_word = train_data.dic.stringFromLongtensor(gen_out)
    print("condition %d : %s"%(c,gen_word))
	
def demo_test(encoder, decoder, test_data):
    encoder.eval()
    decoder.eval()
    score = 0
    output_list = []
    for _ in range(500):
        score_tmp = 0
        output_temp = []
        for i in range(10):
            inputs, input_c, output, output_c = test_data[i]

            z, m, logvar = encoder(inputs[1:-1].to(device), input_c)

            outputs = Decoder(decoder, z, output_c, False, output.to(device))

            outputs_onehot = torch.max(torch.softmax(outputs, dim=1), 1)[1]
            inputs_str = train_data.dic.stringFromLongtensor(inputs, check_end=True)
            target_str = train_data.dic.stringFromLongtensor(output, check_end=True)
            outputs_str = train_data.dic.stringFromLongtensor(outputs_onehot, check_end=True)

            output_temp.append([inputs_str, target_str, outputs_str])

            score_tmp += compute_bleu(outputs_str, target_str)
        score_tmp = score_tmp/10
        if score_tmp > score:
            output_list = output_temp.copy()
            score = score_tmp
    for result in output_list:
        print('{:12s}->{:12s} and the pred output:{}'.format(result[0], result[1], result[2]))
    print('bleu score : %f'%(score))
    
    return score
	
demo_test(encoder, decoder, test_data)

noise = encoder.sample_z()
print(noise)
for i in range(4):
    outputs = generate_word(decoder, noise, i)
    output_str = train_data.dic.stringFromLongtensor(outputs)
    print("condition %d : %s"%(i,output_str))